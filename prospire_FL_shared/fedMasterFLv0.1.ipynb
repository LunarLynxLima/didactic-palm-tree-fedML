{"cells":[{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T22:54:30.296963Z","iopub.status.busy":"2024-07-12T22:54:30.296550Z","iopub.status.idle":"2024-07-12T22:54:30.301958Z","shell.execute_reply":"2024-07-12T22:54:30.300762Z","shell.execute_reply.started":"2024-07-12T22:54:30.296931Z"},"trusted":true},"outputs":[],"source":["# ! pip install -q tensorflow_federated"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T22:54:30.319711Z","iopub.status.busy":"2024-07-12T22:54:30.318732Z","iopub.status.idle":"2024-07-12T22:54:30.347406Z","shell.execute_reply":"2024-07-12T22:54:30.346206Z","shell.execute_reply.started":"2024-07-12T22:54:30.319676Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/prospice/prospire_FL_shared/MasterFL.py\n","/kaggle/input/prospice/prospire_FL_shared/train_y_images_rti.npy\n","/kaggle/input/prospice/prospire_FL_shared/train_x_images_rti.npy\n","/kaggle/input/prospice/prospire_FL_shared/test_x_images_rti.npy\n","/kaggle/input/prospice/prospire_FL_shared/readme.txt\n","/kaggle/input/prospice/prospire_FL_shared/test_y_images_rti.npy\n","/kaggle/input/prospice/prospire_FL_shared/Parameters.py\n","/kaggle/input/prospice/prospire_FL_shared/UnetClass.py\n","/kaggle/input/prospice/prospire_FL_shared/unet_history.json\n","/kaggle/input/prospice/prospire_FL_shared/fedMasterFLv0.1.ipynb\n","/kaggle/input/prospice/prospire_FL_shared/.idea/prospire_secon_version_shared.iml\n","/kaggle/input/prospice/prospire_FL_shared/.idea/.gitignore\n","/kaggle/input/prospice/prospire_FL_shared/.idea/modules.xml\n","/kaggle/input/prospice/prospire_FL_shared/.idea/workspace.xml\n","/kaggle/input/prospice/prospire_FL_shared/.idea/misc.xml\n","/kaggle/input/prospice/prospire_FL_shared/.idea/vcs.xml\n","/kaggle/input/prospice/prospire_FL_shared/.idea/inspectionProfiles/profiles_settings.xml\n","/kaggle/input/prospice/prospire_FL_shared/logs/fit/Unet20240711-042816/validation/events.out.tfevents.1720652301.bluebot.25158.3.v2\n","/kaggle/input/prospice/prospire_FL_shared/logs/fit/Unet20240711-042816/train/events.out.tfevents.1720652297.bluebot.25158.2.v2\n","/kaggle/input/prospice/prospire_FL_shared/logs/fit/Unet20240703-143403/validation/events.out.tfevents.1719997464.DESKTOP-R4J0E0M.36280.1.v2\n","/kaggle/input/prospice/prospire_FL_shared/logs/fit/Unet20240703-143403/train/events.out.tfevents.1719997444.DESKTOP-R4J0E0M.36280.0.v2\n","/kaggle/input/prospice/prospire_FL_shared/logs/fit/Unet20240711-042440/validation/events.out.tfevents.1720652087.bluebot.25158.1.v2\n","/kaggle/input/prospice/prospire_FL_shared/logs/fit/Unet20240711-042440/train/events.out.tfevents.1720652080.bluebot.25158.0.v2\n"]}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T22:54:30.350122Z","iopub.status.busy":"2024-07-12T22:54:30.349686Z","iopub.status.idle":"2024-07-12T22:54:30.360256Z","shell.execute_reply":"2024-07-12T22:54:30.359055Z","shell.execute_reply.started":"2024-07-12T22:54:30.350082Z"},"trusted":true},"outputs":[],"source":["import math\n","\n","\n","class Parameters:\n","\n","    def __init__(self, dataset):\n","        self.dataset = dataset\n","\n","        if dataset == 'wi_outdoor_15_buildings_8_reflections':\n","            self.power_flies_dir = 'WIOutdoor15Buildings8ReflectionsRxPowerFiles/'\n","            self.tx_locs_dir = 'WIOutdoor15Buildings8ReflectionsTxRxLocations/txset.txrx'\n","            self.rx_locs_dir = 'WIOutdoor15Buildings8ReflectionsTxRxLocations/rxset.txrx'\n","            self.buildings_file = 'WIOutdoor15Buildings8ReflectionsBuildingVertices/ConcreteBuildings.object'\n","            self.foliage_file = None\n","            self.xy_grid_rx = True\n","            self.x_min, self.y_min, self.x_max, self.y_max = 0.0, 0.0, 500.0, 500.0\n","            self.cell_width = 10.0\n","            # Total number of TXs is 152 and RXs is 2340\n","            self.num_sensors_list = range(10, 315, 50)\n","            # self.num_train_tx_pos_list = range(10, 95, 20)\n","            self.num_train_tx_pos_list = range(70, 75, 20)\n","            self.num_train_tx_pos = 90\n","            self.num_sensors = 200\n","            self.num_exp_test_tx = 30  # Number of experimental TX positions to test\n","            self.num_target_locs_for_radio_map = 200\n","            self.rss_min = -109.0  # Minimum value of acceptable rss\n","            self.rss_max = -2.0  # Maximum value of acceptable rss\n","            self.valid_rss = self.rss_min\n","            self.clip_low_rss = True\n"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T23:03:11.195362Z","iopub.status.busy":"2024-07-12T23:03:11.194871Z","iopub.status.idle":"2024-07-12T23:03:11.241222Z","shell.execute_reply":"2024-07-12T23:03:11.239981Z","shell.execute_reply.started":"2024-07-12T23:03:11.195326Z"},"trusted":true},"outputs":[],"source":["import timeit\n","\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Input, Dropout, Flatten, Conv1D, GlobalAveragePooling1D, MaxPooling1D, \\\n","    GlobalMaxPooling1D, LeakyReLU, BatchNormalization, Conv2D, MaxPool2D, Conv2DTranspose, Concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras import optimizers\n","from tensorflow.python.platform import gfile\n","# import tensorflow.contrib.tensorrt as trt\n","# from tensorflow.python.framework.graph_util import convert_variables_to_constants\n","from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n","from tensorflow.python.platform import gfile\n","from tensorflow.keras import backend as K\n","# import KerasModelToFrozenGraph as freeze\n","import numpy as np\n","import json, itertools, random\n","from sklearn.preprocessing import StandardScaler\n","from matplotlib import pyplot as plt\n","import tensorflow as tf\n","import math, datetime\n","\n","\n","class UnetClass:\n","\n","    def __init__(self, params, img_height, img_width, img_depth, var_loss=False, ):\n","        self.img_height, self.img_width, self.img_depth = img_height, img_width, img_depth\n","        self.para = params\n","        self.min_rss = params.rss_min; self.max_rss = params.rss_max\n","        self.image_preprocessing = None     # 'normalize' will cause problem with custom loss\n","        if self.image_preprocessing == 'normalize':\n","            self.datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n","        self.normalize_rss = True   # If not using this need to check that predicted values is within range\n","        self.rescale = 1.0 / (self.max_rss - self.min_rss)\n","        self.var_loss = var_loss\n","\n","        self.use_custom_loss = True\n","        self.model = self.unet_architecture()\n","        self.history_file = \"unet_history.json\"; self.checkpoint_model = 'unet_best_model.h5'\n","        self.model_file = 'unet_model.h5'\n","        self.best_batch_size, self.best_epochs = 32, 1000    # model parameters for the nn\n","        self.worker_epochs = 1000, 10\n","        # self.best_batch_size, self.best_epochs = 32, 1000    # model parameters for the nn\n","        self.es = EarlyStopping(monitor='val_loss', mode='min', patience=100, verbose=1)    # simple early stopping\n","        self.mc = ModelCheckpoint(self.checkpoint_model, monitor='val_loss', mode='min', verbose=0,\n","                                      save_best_only=True) # saves the best model\n","        self.log_dir = \"logs/\" + \"fit/\" + 'Unet' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","        self.tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=self.log_dir, histogram_freq=1)\n","\n","    def unet_architecture(self):\n","        alpha = 0.0  # for leaky relu\n","\n","        def conv_block(input, num_filters, kernel_size):\n","            x = Conv2D(num_filters, kernel_size, padding=\"same\", kernel_initializer='he_normal',\n","                       kernel_regularizer='l2')(input)\n","            x = BatchNormalization()(x)\n","            x = LeakyReLU(alpha=alpha)(x)\n","\n","            x = Conv2D(num_filters, kernel_size, padding=\"same\", kernel_initializer='he_normal',\n","                       kernel_regularizer='l2')(x)\n","            x = BatchNormalization()(x)\n","            x = LeakyReLU(alpha=alpha)(x)\n","            return x\n","\n","        def encoder_block(input, num_filters, kernel_size):\n","            # kernel_size = (3, 3)\n","            x = conv_block(input, num_filters, kernel_size)\n","            p = MaxPool2D((2, 2))(x)\n","            # x = BatchNormalization()(x)\n","            # p = Dropout(rate=0.25)(p)\n","            return x, p\n","\n","        def decoder_block(input, skip_features, num_filters, kernel_size):\n","            x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\", kernel_initializer='he_normal',\n","                                kernel_regularizer='l2')(input)\n","            # x = BatchNormalization()(x)\n","            # x = LeakyReLU(alpha=alpha)(x)\n","            x = Concatenate()([skip_features, x])\n","            # kernel_size = (3, 3)\n","            x = conv_block(x, num_filters, kernel_size)\n","            return x\n","\n","        inputs = Input(shape=(self.img_height - 2, self.img_width - 2, self.img_depth)) # for adjustment from 50 to 48\n","\n","        # Encoder\n","        s1, p1 = encoder_block(inputs, 32, (3, 3))\n","        s2, p2 = encoder_block(p1, 64, (3, 3))\n","        s3, p3 = encoder_block(p2, 128, (3, 3))\n","        s4, p4 = encoder_block(p3, 256, (3, 3))\n","\n","        b1 = conv_block(p4, 256, (3, 3))\n","\n","        d1 = decoder_block(b1, s4, 256, (3, 3))\n","        d2 = decoder_block(d1, s3, 128, (3, 3))\n","        d3 = decoder_block(d2, s2, 64, (3, 3))\n","        d4 = decoder_block(d3, s1, 32, (3, 3))\n","\n","        outputs = Conv2D(1, (1, 1), padding=\"same\", activation=\"relu\")(d4)\n","        model = Model(inputs, outputs, name=\"U-Net\")\n","        adam = optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999)\n","        if self.use_custom_loss:\n","            model.compile(loss=self.custom_loss, optimizer='adam')\n","        else:\n","            model.compile(optimizer='adam', loss=\"mean_absolute_error\")\n","            # model.compile(optimizer='adam', loss=\"mean_squared_error\")\n","        # model.summary()\n","        return model\n","\n","    def custom_loss(self, y_true, y_pred):\n","        img_height = y_true.shape[1]; img_width = y_true.shape[2]\n","\n","        if self.var_loss:\n","            lambda_overestimate = 1.0\n","            lambda_underestimate = 10.0\n","        else:\n","            lambda_overestimate = 1.0\n","            lambda_underestimate = 1.0\n","\n","        y_true_mask = tf.where(y_true < 0.0, 0.0, 1.0)\n","        y_true_mask_flattened = K.reshape(y_true_mask, (-1, img_height * img_width))\n","        y_true_count_nonzero = K.sum(y_true_mask_flattened, axis=-1)\n","\n","        diff_loss_masked = (y_true - y_pred) * y_true_mask\n","\n","        underestimate_loss_mask = tf.where(diff_loss_masked > 0.0, 1.0 * lambda_underestimate, 0.0)\n","        underestimate_loss_mask_flattened = K.reshape(underestimate_loss_mask, (-1, img_height * img_width))\n","        underestimate_count_nonzero = K.sum(underestimate_loss_mask_flattened, axis=-1)\n","\n","        overestimate_loss_mask = tf.where(diff_loss_masked < 0.0, 1.0 * lambda_overestimate, 0.0)\n","        overestimate_loss_mask_flattened = K.reshape(overestimate_loss_mask, (-1, img_height * img_width))\n","        overestimate_count_nonzero = K.sum(overestimate_loss_mask_flattened, axis=-1)\n","\n","        diff_loss_masked = tf.where(diff_loss_masked > 0.0, diff_loss_masked * lambda_underestimate, diff_loss_masked)\n","        diff_loss_masked = tf.where(diff_loss_masked < 0.0, diff_loss_masked * lambda_overestimate, diff_loss_masked)\n","        abs_loss_masked = tf.abs(diff_loss_masked)\n","        loss_masked = abs_loss_masked\n","\n","        loss_masked = K.reshape(loss_masked, (-1, img_height * img_width))\n","        sum_loss_masked = K.sum(loss_masked, axis=-1)\n","        mean_loss_masked = sum_loss_masked / (underestimate_count_nonzero + overestimate_count_nonzero)\n","        # mean_loss_masked = sum_loss_masked / y_true_count_nonzero\n","\n","        return mean_loss_masked\n","\n","    def training(self, train_x, train_y, start_training, worker_epochs = -1):\n","        if not start_training:\n","            self.model = load_model(self.model_file, custom_objects={'custom_loss': self.custom_loss})\n","            return\n","\n","        if self.normalize_rss:\n","            if self.use_custom_loss:\n","                train_y = np.where(train_y == 0.0, self.min_rss - 10.0, train_y)\n","            else:\n","                train_y = np.where(train_y == 0.0, self.min_rss, train_y)\n","            train_y = train_y - self.min_rss\n","            train_y = train_y * self.rescale\n","        # print(train_x.shape, train_y.shape)\n","        train_x = np.transpose(train_x, [0, 2, 3, 1])   # for channel last orientation\n","        train_y = train_y.reshape(tuple(list(train_y.shape) + [1]))\n","        # print(train_x.shape, train_y.shape)\n","        # remove the outermost rows and columns for easier integration with UNET structure\n","        train_x = train_x[:, 1:-1, 1:-1, :]; train_y = train_y[:, 1:-1, 1:-1, :]\n","        print('Train and test shape for UNET', train_x.shape, train_y.shape)\n","        print('Train and test set data size', train_x.nbytes, train_y.nbytes)\n","\n","        if worker_epochs != -1:\n","            self.best_epochs = worker_epochs\n","\n","        if self.image_preprocessing == 'normalize':\n","            train_idxs = np.random.choice(np.arange(train_x.shape[0]), size=int(train_x.shape[0] * 0.8), replace=False)\n","            val_idxs = list(set(np.arange(train_x.shape[0])) - set(train_idxs))\n","\n","            x_val, y_val = train_x[val_idxs, ...], train_y[val_idxs, ...]\n","            train_x, train_y = train_x[train_idxs, ...], train_y[train_idxs, ...]\n","            print('Train_x and train_y shape for UNET', train_x.shape, train_y.shape)\n","            print('val_x and val_y shape for UNET', x_val.shape, y_val.shape)\n","\n","            self.datagen.fit(train_x)\n","\n","            history = self.model.fit(self.datagen.flow(train_x, train_y, batch_size=self.best_batch_size),\n","                                     steps_per_epoch=math.ceil(len(train_x) / float(self.best_batch_size)),\n","                                     epochs=self.best_epochs,\n","                                     callbacks=[self.es, self.mc],\n","                                     validation_data=self.datagen.flow(x_val, y_val, batch_size=self.best_batch_size),\n","                                     validation_steps=math.ceil(len(x_val) / float(self.best_batch_size)),\n","                                     verbose=0)\n","\n","        else:\n","            history = self.model.fit(train_x, train_y,\n","                                     validation_split=0.2,\n","                                     batch_size=self.best_batch_size,\n","                                     epochs=self.best_epochs,\n","                                     callbacks=[self.es, self.mc, self.tensorboard_callback],\n","                                     verbose=1,\n","                                     shuffle=True)\n","\n","        if self.use_custom_loss:\n","            self.model = load_model(self.checkpoint_model, custom_objects={'custom_loss': self.custom_loss})\n","        else:\n","            self.model.load_weights(self.checkpoint_model)  # load the saved best model\n","        self.model.save(self.model_file)        # Save the model for future use\n","        with open(self.history_file, \"w\") as json_file:     # save the history in a file\n","            json.dump(history.history, json_file)\n","#         print('best_val_loss, best_train_loss', self.get_train_stats())\n","\n","    # Returns the validation loss and train loss for the chosen model\n","    def get_train_stats(self):\n","        with open(self.history_file, \"r\") as json_file:\n","            history = json.load(json_file)\n","        best_val_loss = min(history['val_loss'])\n","        i, = np.where(np.array(history['val_loss']) == best_val_loss)\n","        best_idx = i[0]\n","        best_train_loss = history['loss'][best_idx]\n","        return best_val_loss, best_train_loss\n","\n","    def predict_rss(self, test_x, batch=False):\n","        if len(test_x.shape) == 3:\n","            test_x = test_x.reshape(tuple([1] + list(test_x.shape)))\n","        test_x = np.transpose(test_x, [0, 2, 3, 1])     # for making channels last\n","        test_x = test_x[:, 1:-1, 1:-1, :]       # for making images 48x48\n","\n","        if self.image_preprocessing == 'normalize':\n","            test_iterator = self.datagen.flow(test_x, None, batch_size=1)\n","            test_x = test_iterator.next()\n","\n","        tx = timeit.default_timer()\n","        if batch:\n","            predicted_values = self.model.predict(test_x, batch_size=1024, verbose=0)\n","        else:\n","            # predicted_values = self.model.predict(test_x, verbose=0)\n","            predicted_values = self.model(test_x, training=False)\n","        # print(timeit.default_timer() - tx)\n","\n","        if self.normalize_rss:\n","            rss_values = predicted_values / self.rescale\n","            rss_values = rss_values + self.min_rss\n","        else:\n","            rss_values = predicted_values\n","        rss_values = np.where(test_x[..., 1] == 0.0, 0.0, rss_values[..., 0])  # use predictions only for active RX locs\n","        # make the images back to 50x50\n","        temp_res = np.zeros((rss_values.shape[0], self.img_height, self.img_width))\n","        temp_res[:, 1:-1, 1:-1] = rss_values\n","        rss_values = temp_res\n","        if not batch:\n","            return rss_values[0, ...]\n","        else:\n","            return rss_values"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T23:03:17.401981Z","iopub.status.busy":"2024-07-12T23:03:17.401560Z","iopub.status.idle":"2024-07-12T23:03:18.098125Z","shell.execute_reply":"2024-07-12T23:03:18.096302Z","shell.execute_reply.started":"2024-07-12T23:03:17.401948Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["number of sub-data : 70;  train_x_images[0].shape :  (4, 50, 50) ; train_y_images[0].shape :  (50, 50) ; \n","test_x_images.shape :  (30, 4, 50, 50) ; test_y_images.shape :  (30, 50, 50)\n"]},{"ename":"ValueError","evalue":"The filepath provided must end in `.keras` (Keras model format). Received: filepath=unet_best_model.h5","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[61], line 177\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest error\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39msum(temp_err) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mcount_nonzero(temp_err))\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 177\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m## main code base ends here\u001b[39;00m\n","Cell \u001b[0;32mIn[61], line 144\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPost augmentation:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_x_images.shape, train_y_images.shape, test_x_images.shape, test_y_images.shape\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    142\u001b[0m           train_x_images\u001b[38;5;241m.\u001b[39mshape, train_y_images\u001b[38;5;241m.\u001b[39mshape, test_x_images\u001b[38;5;241m.\u001b[39mshape, test_y_images\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 144\u001b[0m global_nunet_obj_rti \u001b[38;5;241m=\u001b[39m \u001b[43mUnetClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpara\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_pixels_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_pixels_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_x_images\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m workers_nunet_obj_rti \u001b[38;5;241m=\u001b[39m [UnetClass(para, num_pixels_x, num_pixels_y, train_x_images[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_workers)]\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# intialize workers\u001b[39;00m\n","Cell \u001b[0;32mIn[60], line 49\u001b[0m, in \u001b[0;36mUnetClass.__init__\u001b[0;34m(self, params, img_height, img_width, img_depth, var_loss)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# self.best_batch_size, self.best_epochs = 32, 1000    # model parameters for the nn\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mes \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)    \u001b[38;5;66;03m# simple early stopping\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmc \u001b[38;5;241m=\u001b[39m \u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# saves the best model\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnet\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensorboard_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir, histogram_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:191\u001b[0m, in \u001b[0;36mModelCheckpoint.__init__\u001b[0;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    192\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filepath provided must end in `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Keras model format). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         )\n","\u001b[0;31mValueError\u001b[0m: The filepath provided must end in `.keras` (Keras model format). Received: filepath=unet_best_model.h5"]}],"source":["import numpy as np\n","import timeit, time\n","# from UnetClass import UnetClass\n","# from Parameters import Parameters\n","from matplotlib import pyplot as plt\n","\n","dataset = 'wi_outdoor_15_buildings_8_reflections'  # dataset to use. For other available datasets, see Parameters.py\n","para = Parameters(dataset)\n","num_pixels_x = int((para.x_max - para.x_min) / para.cell_width)  # image width\n","num_pixels_y = int((para.y_max - para.y_min) / para.cell_width)  # image height\n","# for sampling RXs and creating images\n","num_samples = 40\n","num_images = 200\n","\n","gen_plots = False\n","use_aug = False\n","\n","start_training = True\n","n_workers = 4 # number of workers to use for federated learning :: 1 means traditional learning without FL\n","\n","def augmentation(train_x_images, train_y_images):\n","    train_x_images_aug = []; train_y_images_aug = []\n","    for i in range(train_x_images.shape[0]):\n","        tx_img = train_x_images[i, 0, ...]\n","        rx_img = train_x_images[i, 1, ...]\n","        map_img = train_x_images[i, 2, ...]\n","        rti_img = train_x_images[i, 3, ...]\n","        rss_img = train_y_images[i]\n","\n","        non_zero_pixels = np.nonzero(rx_img)\n","        non_zero_pixels = np.array(list(zip(non_zero_pixels[0], non_zero_pixels[1])))\n","\n","        for j in range(num_images):\n","            selected_pixels = np.random.choice(non_zero_pixels.shape[0], size=num_samples, replace=False)\n","            selected_pixels = non_zero_pixels[selected_pixels, :]\n","            selected_pixels = tuple(list(zip(*selected_pixels)))\n","\n","            rx_img_new = np.zeros_like(rx_img)\n","            rx_img_new[selected_pixels] = rx_img[selected_pixels]\n","            rti_img_new = np.zeros_like(rti_img)\n","            rti_img_new[selected_pixels] = rti_img[selected_pixels]\n","            new_data = np.stack([tx_img, rx_img_new, map_img, rti_img_new], axis=0)\n","            train_x_images_aug.append(new_data)\n","\n","            rss_img_new = np.zeros_like(rss_img)\n","            rss_img_new[selected_pixels] = rss_img[selected_pixels]\n","            train_y_images_aug.append(rss_img_new)\n","\n","            # plotting some sample images\n","            if gen_plots:\n","                fig, axs = plt.subplots(nrows=2, ncols=5, constrained_layout=True)\n","                plot_data = [tx_img, rx_img, map_img, rti_img, rss_img, new_data[0, ...], new_data[1, ...],\n","                             new_data[2, ...], new_data[3, ...], rss_img_new]\n","                titles = ['TX', 'RX', 'Map', 'RTI', 'RSS', 'TX', 'RX_samp', 'Map', 'RTI_samp', 'RSS_samp']\n","                for idx, ax in enumerate(axs.flat):\n","                    ax.grid(True)\n","                    vec = plot_data[idx]; title = titles[idx]\n","\n","                    im = ax.imshow(vec, aspect='auto', cmap='viridis', interpolation=\"nearest\")\n","                    cbar = fig.colorbar(im, ax=ax, orientation=\"vertical\", shrink=0.99, aspect=40, pad=0.01)\n","                    cbar.ax.tick_params()\n","\n","                    ax.set_xticks(np.arange(0, 50, 5) - 0.5); ax.set_xticklabels(10 * np.arange(0, 50, 5), rotation=45)\n","                    ax.set_yticks(np.arange(0, 50, 5) + 0.5); ax.set_yticklabels(10 * np.flip(np.arange(0, 50, 5)))\n","                    # ax.tick_params(labelsize=labelsize)\n","                    ax.set_title(title)\n","                plt.show()\n","    return [np.array(train_x_images_aug), np.array(train_y_images_aug)]\n","\n","def federated_averaging(global_model, worker_models, weights):\n","    global_weights = global_model.get_weights()\n","    num_workers = len(worker_models)\n","    \n","    # Initialize averaged weights\n","    new_weights = [np.zeros_like(w) for w in global_weights]\n","    \n","    # Weighted average of local models' weights\n","    for i, worker_model in enumerate(worker_models):\n","        local_weights = worker_model.get_weights()\n","        for j in range(len(new_weights)):\n","            new_weights[j] += local_weights[j] * weights[i]\n","    \n","    # Set the averaged weights to the global model\n","    global_model.set_weights(new_weights)\n","    # return global_model\n","\n","class FederatedWorker:\n","    def __init__(self, worker_id, nunet_obj, train_x_images, train_y_images, params, worker_epochs=100, total_workers=4):\n","        self.worker_id = worker_id\n","        self.nunet_obj = nunet_obj\n","        self.model = nunet_obj.model\n","        self.train_x_images = train_x_images * gen_mask(train_x_images, n=total_workers)\n","        self.train_y_images = train_y_images\n","        self.worker_epochs = worker_epochs\n","        self.params = params\n","    \n","    def gen_mask(tensor, prob=None, n=None, dtype=torch.float32):\n","        \"\"\"\n","        batched mask \n","        is all one for idx 0,3 as per 0 is TX and 3 is location map\n","        and for idx 1 and 2 is 1 for sampled points with probability of 1/n or prob as as prompted\n","        \"\"\"\n","        if (prob is not None) and (n is not None):\n","            return -1\n","        if n is not None:\n","            prob = 1 / n\n","        mask = np.ones(tensor.shape)\n","        if len(tensor.shape) == 4:\n","            for i in range(tensor.shape[0]):\n","                mask[i, 1] = np.random.choice([0, 1], size=tensor.shape[2:], p=[1-prob, prob])\n","                mask[i, 3] = np.random.choice([0, 1], size=tensor.shape[2:], p=[1-prob, prob])\n","        else:\n","            raise ValueError(\"Tensor must be a 4D batch of 3D tensors.\")\n","        return torch.tensor(mask, dtype=dtype)\n","\n","    def train_local_model(self):\n","        # Train local model and return training time, val_loss, and train_loss\n","        start_time = time.time()\n","        self.nunet_obj.training(self.train_x_images, self.train_y_images, start_training=True, worker_epochs=self.worker_epochs)\n","        training_time = time.time() - start_time\n","#         val_loss, train_loss = self.model.get_train_stats()\n","#         return val_loss, train_loss, training_time\n","    \n","\n","def main():\n","    train_x_images, train_y_images = np.load('/kaggle/input/prospice/prospire_FL_shared/train_x_images_rti.npy', 'r'), np.load('/kaggle/input/prospice/prospire_FL_shared/train_y_images_rti.npy', 'r')\n","    test_x_images, test_y_images = np.load('/kaggle/input/prospice/prospire_FL_shared/test_x_images_rti.npy', 'r'), np.load('/kaggle/input/prospice/prospire_FL_shared/test_y_images_rti.npy', 'r')\n","    \n","    train_x_images_tensor = torch.tensor(train_x_images, dtype=torch.float32)\n","\n","#     train_x_images, train_y_images = np.array_split(r_train_x_images, n_workers), np.array_split(r_train_y_images, n_workers)\n","    \n","    print(f'number of sub-data : {len(train_y_images)}; ', \n","          'train_x_images[0].shape : ', train_x_images[0].shape, '; train_y_images[0].shape : ', train_y_images[0].shape, \n","          '; \\ntest_x_images.shape : ', test_x_images.shape, '; test_y_images.shape : ', test_y_images.shape)    \n","\n","    if use_aug:\n","        train_x_images, train_y_images = augmentation(train_x_images, train_y_images)\n","        test_x_images, test_y_images = augmentation(test_x_images, test_y_images)\n","        print('Post augmentation:')\n","        print('train_x_images.shape, train_y_images.shape, test_x_images.shape, test_y_images.shape',\n","              train_x_images.shape, train_y_images.shape, test_x_images.shape, test_y_images.shape)\n","\n","    global_nunet_obj_rti = UnetClass(para, num_pixels_x, num_pixels_y, train_x_images[-1].shape[1], False)\n","    workers_nunet_obj_rti = [UnetClass(para, num_pixels_x, num_pixels_y, train_x_images[-1].shape[1], False) for i in range(n_workers)]\n","\n","    # intialize workers\n","    w_epcochs = 1\n","    workers = [FederatedWorker(i, workers_nunet_obj_rti[i], train_x_images_tensor, train_y_images, para, worker_epochs = w_epcochs, total_workers = n_workers) for i in range(n_workers)]\n","    epochs = 1 #int(1000//100) # UnetClass.best_epochs\n","    for epoch in range(epochs):\n","        print(f'Epoch {epoch+1}')\n","        \n","        # Train local models\n","        for worker in workers:\n","            worker.train_local_model()\n","        \n","        # Federated averaging\n","        federated_averaging(global_nunet_obj_rti.model, [worker_obj.model for worker_obj in workers],  [1/n_workers]*n_workers)\n","\n","    # compute train error\n","    temp_pred = global_nunet_obj_rti.predict_rss(train_x_images, True)\n","    temp_err = np.where(train_y_images == 0.0, train_y_images, np.abs(temp_pred - train_y_images))\n","    train_error = (np.sum(temp_err)) / np.count_nonzero(temp_err)\n","    print('train_error', train_error)\n","    # prediction\n","    nunet_rti_prediction = []\n","    for item, vals in zip(test_x_images, test_y_images):\n","        temp_unet = global_nunet_obj_rti.predict_rss(item)\n","        nunet_rti_prediction.append(temp_unet)\n","    nunet_rti_prediction = np.reshape(np.array(nunet_rti_prediction), test_y_images.shape)\n","    temp_err = np.where(test_y_images == 0.0, test_y_images, np.abs(nunet_rti_prediction - test_y_images))\n","    print('test error', np.sum(temp_err) / np.count_nonzero(temp_err))\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n","## main code base ends here"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-12T22:54:33.174778Z","iopub.status.idle":"2024-07-12T22:54:33.175199Z","shell.execute_reply":"2024-07-12T22:54:33.175013Z","shell.execute_reply.started":"2024-07-12T22:54:33.174996Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","train_x_images, train_y_images = np.load('/kaggle/input/prospice/prospire_FL_shared/train_x_images_rti.npy', 'r'), np.load('/kaggle/input/prospice/prospire_FL_shared/train_y_images_rti.npy', 'r')\n","test_x_images, test_y_images = np.load('/kaggle/input/prospice/prospire_FL_shared/test_x_images_rti.npy', 'r'), np.load('/kaggle/input/prospice/prospire_FL_shared/test_y_images_rti.npy', 'r')\n","# train_x_images, train_y_images = np.array_split(r_train_x_images, n_workers), np.array_split(r_train_y_images, n_workers)\n","\n","# print(f'number of sub-data : {len(train_y_images)}; ', \n","#       'train_x_images[0].shape : ', train_x_images.shape, '; train_y_images[0].shape : ', train_y_images.shape, \n","#       '; \\ntest_x_images.shape : ', test_x_images.shape, '; test_y_images.shape : ', test_y_images.shape)    \n","\n","# print((train_x_images[0][0]))\n","# print((train_x_images[0][1]))\n","# print((train_x_images[0][2]))\n","# print((train_x_images[0][3]))\n","\n","# for tensor in train_x_images[0]:\n","#     plt.imshow(tensor, cmap='viridis')\n","#     plt.colorbar()\n","#     plt.title('Tensor Image')\n","#     plt.show()\n","\n","\n","def gen_mask(tensor, prob=None, n=None, dtype=torch.float32):\n","    if (prob is not None) and (n is not None):\n","        return -1\n","    if n is not None:\n","        prob = 1 / n\n","        \n","    mask = np.ones(tensor.shape)\n","    \n","    # Generate a mask for the 2nd and 4th elements along the second dimension for each tensor in the batch\n","    if len(tensor.shape) == 4:\n","        for i in range(tensor.shape[0]):\n","            mask[i, 1] = np.random.choice([0, 1], size=tensor.shape[2:], p=[1-prob, prob])\n","            mask[i, 3] = np.random.choice([0, 1], size=tensor.shape[2:], p=[1-prob, prob])\n","    else:\n","        raise ValueError(\"Tensor must be a 4D batch of 3D tensors.\")\n","    \n","    return torch.tensor(mask, dtype=dtype)\n","\n","# Step 1: Create a batch of 3D tensors\n","batch_size = 100\n","p = torch.tensor(np.random.rand(batch_size, 4, 50, 50), dtype=torch.float32)\n","\n","for tenso in tensor[0][1:2]:\n","    plt.imshow(p[0][2], cmap='viridis')\n","    plt.colorbar()\n","    plt.title('Tensor Image')\n","    plt.show()\n","    break\n","n = 2\n","mask_tensor = gen_mask(tensor, n=n)\n","sampled_tensor = tensor * mask_tensor\n","\n","# print(\"Original Tensor:\")\n","# print(tensor)\n","# print(\"\\nMask:\")\n","# print(mask_tensor)\n","# print(\"\\nSampled Tensor:\")\n","# print(sampled_tensor)\n","for pa in tensor[0][1:2]:\n","    plt.imshow(sampled_tensor[0][1], cmap='viridis')\n","    plt.colorbar()\n","    plt.title('Tensor Image')\n","    plt.show()\n","    break"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5371028,"sourceId":8928794,"sourceType":"datasetVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
